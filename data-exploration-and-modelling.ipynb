{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom pathlib import Path\nfrom colorama import Fore, Back, Style\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display, Image\nimport ast\n\n\nROOT_DIR = Path(\"/kaggle/input/tensorflow-great-barrier-reef\")\n\nTRAIN_CSV = ROOT_DIR / \"train.csv\"\nTRAIN_DF = pd.read_csv(TRAIN_CSV)\n\nTEST_CSV = ROOT_DIR / \"test.csv\"\nTEST_DF = pd.read_csv(TEST_CSV)\n\nlist(ROOT_DIR.iterdir())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-02T16:24:36.067685Z","iopub.execute_input":"2022-06-02T16:24:36.068345Z","iopub.status.idle":"2022-06-02T16:24:36.431940Z","shell.execute_reply.started":"2022-06-02T16:24:36.068244Z","shell.execute_reply":"2022-06-02T16:24:36.431166Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:36.433684Z","iopub.execute_input":"2022-06-02T16:24:36.434187Z","iopub.status.idle":"2022-06-02T16:24:36.452214Z","shell.execute_reply.started":"2022-06-02T16:24:36.434149Z","shell.execute_reply":"2022-06-02T16:24:36.451463Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[\"video_id\"].unique()\n# Checking for unique video_id","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:36.454885Z","iopub.execute_input":"2022-06-02T16:24:36.455445Z","iopub.status.idle":"2022-06-02T16:24:36.465565Z","shell.execute_reply.started":"2022-06-02T16:24:36.455392Z","shell.execute_reply":"2022-06-02T16:24:36.464700Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[\"sequence\"].unique()\n# Checking for unique sequence ids","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:36.594646Z","iopub.execute_input":"2022-06-02T16:24:36.595172Z","iopub.status.idle":"2022-06-02T16:24:36.602301Z","shell.execute_reply.started":"2022-06-02T16:24:36.595130Z","shell.execute_reply":"2022-06-02T16:24:36.601516Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vid_0 = TRAIN_DF[TRAIN_DF[\"video_id\"] == 0][\"sequence\"].unique()\nprint(vid_0)\nprint(vid_0.shape[0])\n\n# Printing the unique sequence_id for video_id 0","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:36.737557Z","iopub.execute_input":"2022-06-02T16:24:36.738186Z","iopub.status.idle":"2022-06-02T16:24:36.749366Z","shell.execute_reply.started":"2022-06-02T16:24:36.738149Z","shell.execute_reply":"2022-06-02T16:24:36.748316Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"vid_1 = TRAIN_DF[TRAIN_DF[\"video_id\"] == 1][\"sequence\"].unique()\nprint(vid_1)\nprint(vid_1.shape[0])\n\n# Printing the unique sequence_id for video_id 1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:36.863581Z","iopub.execute_input":"2022-06-02T16:24:36.864167Z","iopub.status.idle":"2022-06-02T16:24:36.872945Z","shell.execute_reply.started":"2022-06-02T16:24:36.864124Z","shell.execute_reply":"2022-06-02T16:24:36.871984Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"vid_2 = TRAIN_DF[TRAIN_DF[\"video_id\"] == 2][\"sequence\"].unique()\nprint(vid_2)\nprint(vid_2.shape[0])\n\n# Printing the unique sequence_id for video_id 2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.000730Z","iopub.execute_input":"2022-06-02T16:24:37.001550Z","iopub.status.idle":"2022-06-02T16:24:37.010068Z","shell.execute_reply.started":"2022-06-02T16:24:37.001508Z","shell.execute_reply":"2022-06-02T16:24:37.009190Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if TRAIN_DF[\"sequence\"].unique().shape[0] == (vid_0.shape[0] + vid_1.shape[0] + vid_2.shape[0]):\n    print(\"Sequence numbers are unique in 3 videos\")\nelse:\n    print(\"Duplicate sequence numbers are found in 3 videos\")\n    \n# Checking for duplicate sequence numbers","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.134100Z","iopub.execute_input":"2022-06-02T16:24:37.134730Z","iopub.status.idle":"2022-06-02T16:24:37.140833Z","shell.execute_reply.started":"2022-06-02T16:24:37.134691Z","shell.execute_reply":"2022-06-02T16:24:37.140065Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[TRAIN_DF[\"video_id\"] == 0].shape\n\n# Checking the number of images under video_id 0","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.268899Z","iopub.execute_input":"2022-06-02T16:24:37.269391Z","iopub.status.idle":"2022-06-02T16:24:37.279081Z","shell.execute_reply.started":"2022-06-02T16:24:37.269353Z","shell.execute_reply":"2022-06-02T16:24:37.278329Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[TRAIN_DF[\"video_id\"] == 1].shape\n\n# Checking the number of images under video_id 1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.420352Z","iopub.execute_input":"2022-06-02T16:24:37.420615Z","iopub.status.idle":"2022-06-02T16:24:37.430179Z","shell.execute_reply.started":"2022-06-02T16:24:37.420587Z","shell.execute_reply":"2022-06-02T16:24:37.429410Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[TRAIN_DF[\"video_id\"] == 2].shape\n\n# Checking the number of images under video_id 2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.556260Z","iopub.execute_input":"2022-06-02T16:24:37.557892Z","iopub.status.idle":"2022-06-02T16:24:37.566324Z","shell.execute_reply.started":"2022-06-02T16:24:37.557839Z","shell.execute_reply":"2022-06-02T16:24:37.565438Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def print_record_length(vid_id):\n    seqs = TRAIN_DF[TRAIN_DF[\"video_id\"] == vid_id][\"sequence\"].unique()\n    dict_seqid_to_cnt = {}\n    for seq_id in seqs:\n        train_df = TRAIN_DF[TRAIN_DF[\"video_id\"] == vid_id]\n        train_df = train_df[train_df[\"sequence\"] == seq_id]\n        dict_seqid_to_cnt[seq_id] = len(train_df)\n    return dict_seqid_to_cnt\n\n# Returning the dictionary with key as sequence_id of corresponding vid_id and value as number of images under this seq_id","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.687179Z","iopub.execute_input":"2022-06-02T16:24:37.689290Z","iopub.status.idle":"2022-06-02T16:24:37.696039Z","shell.execute_reply.started":"2022-06-02T16:24:37.689249Z","shell.execute_reply":"2022-06-02T16:24:37.695221Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dict_seqid_to_cnt =  print_record_length(2)\nfor seqid in vid_2:\n    print(\"Video ID with \" + str(2) + \" with sequence number \" + str(seqid) + \" having \" + str(dict_seqid_to_cnt[seqid]) + \" number of records\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.818149Z","iopub.execute_input":"2022-06-02T16:24:37.818855Z","iopub.status.idle":"2022-06-02T16:24:37.834035Z","shell.execute_reply.started":"2022-06-02T16:24:37.818813Z","shell.execute_reply":"2022-06-02T16:24:37.833073Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def check_annotations_present(vid_id):\n    seqs = TRAIN_DF[TRAIN_DF[\"video_id\"] == vid_id][\"sequence\"].unique()\n    dict_seq_id_to_idxs = {}\n    for seq_id in seqs:\n        idxs = []\n        train_df = TRAIN_DF[TRAIN_DF[\"video_id\"] == vid_id]\n        train_df = train_df[train_df[\"sequence\"] == seq_id]\n        for i in range(len(train_df)):\n            list_of_dict_ann = ast.literal_eval(train_df.iloc[i].annotations)\n            if len(list_of_dict_ann) > 0 :\n                idxs.append(train_df.index[i])\n        \n        dict_seq_id_to_idxs[seq_id] = idxs\n    return dict_seq_id_to_idxs\n\n# Returning the dictionary with key as sequence_id of corresponding vid_id and value as the list of number of images under this seq_id","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:37.951396Z","iopub.execute_input":"2022-06-02T16:24:37.951980Z","iopub.status.idle":"2022-06-02T16:24:37.963225Z","shell.execute_reply.started":"2022-06-02T16:24:37.951934Z","shell.execute_reply":"2022-06-02T16:24:37.962384Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dict_seq_id_to_idxs = check_annotations_present(2)\nfor idx in vid_2:\n    print(\"Sequence id with \" + str(idx) + \" having \" + str(len(dict_seq_id_to_idxs[idx])) + \" annotated images out of \" + str(dict_seqid_to_cnt[idx]) + \" images\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:38.080215Z","iopub.execute_input":"2022-06-02T16:24:38.080788Z","iopub.status.idle":"2022-06-02T16:24:39.379949Z","shell.execute_reply.started":"2022-06-02T16:24:38.080705Z","shell.execute_reply":"2022-06-02T16:24:39.379225Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.384361Z","iopub.execute_input":"2022-06-02T16:24:39.386362Z","iopub.status.idle":"2022-06-02T16:24:39.411405Z","shell.execute_reply.started":"2022-06-02T16:24:39.386322Z","shell.execute_reply":"2022-06-02T16:24:39.410681Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Image_id = video_id + video_frame","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.415656Z","iopub.execute_input":"2022-06-02T16:24:39.417867Z","iopub.status.idle":"2022-06-02T16:24:39.423225Z","shell.execute_reply.started":"2022-06-02T16:24:39.417825Z","shell.execute_reply":"2022-06-02T16:24:39.422241Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Is the sequence_frame the same as video_frame?","metadata":{}},{"cell_type":"code","source":"df = TRAIN_DF[TRAIN_DF[\"video_id\"] == 2]\ndf1 = df[df[\"sequence\"] == vid_2[0]]\ndf1\n\n# Sequences under video_id 0","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.428882Z","iopub.execute_input":"2022-06-02T16:24:39.431532Z","iopub.status.idle":"2022-06-02T16:24:39.458394Z","shell.execute_reply.started":"2022-06-02T16:24:39.431486Z","shell.execute_reply":"2022-06-02T16:24:39.457651Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df2 = df[df[\"sequence\"] == vid_2[1]]\ndf2\n\n# Sequences under video_id 1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.462561Z","iopub.execute_input":"2022-06-02T16:24:39.464669Z","iopub.status.idle":"2022-06-02T16:24:39.487955Z","shell.execute_reply.started":"2022-06-02T16:24:39.464630Z","shell.execute_reply":"2022-06-02T16:24:39.487202Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df3 = df[df[\"sequence\"] == vid_2[2]]\ndf3\n\n# Sequences under video_id 2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.492049Z","iopub.execute_input":"2022-06-02T16:24:39.494129Z","iopub.status.idle":"2022-06-02T16:24:39.518317Z","shell.execute_reply.started":"2022-06-02T16:24:39.494089Z","shell.execute_reply":"2022-06-02T16:24:39.517549Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# By observing the data, we can come to conclusion that\n# -> sequence_frame starts from 0 for every sequence till the length of the sequence (n)\n# -> video_frame starts from random number and is contiguous but not consecutive","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.522677Z","iopub.execute_input":"2022-06-02T16:24:39.524773Z","iopub.status.idle":"2022-06-02T16:24:39.530366Z","shell.execute_reply.started":"2022-06-02T16:24:39.524716Z","shell.execute_reply":"2022-06-02T16:24:39.529391Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Do sequence_frames go from 0 to N nicely?","metadata":{}},{"cell_type":"code","source":"print(\"Sequence frames sequential and start from 0?\")\nfor seq_name in TRAIN_DF[\"sequence\"].unique():\n    sequential = True\n    numbers = TRAIN_DF[TRAIN_DF[\"sequence\"] == seq_name][\"sequence_frame\"].values\n    numbers.sort()\n    \n    i = 0\n    for num in numbers:\n        while i < num:\n            print(f\"Seq {seq_name}: {Fore.RED}Missing {i}{Fore.RESET}\")\n            i += 1\n        i += 1\n\n    if sequential:\n        print(f\"Seq {seq_name}: {Fore.GREEN}Yes{Fore.RESET}\")\nprint()\n\n# Checking for the sequentiality of the sequence_frames","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.535375Z","iopub.execute_input":"2022-06-02T16:24:39.537968Z","iopub.status.idle":"2022-06-02T16:24:39.609480Z","shell.execute_reply.started":"2022-06-02T16:24:39.537913Z","shell.execute_reply":"2022-06-02T16:24:39.608682Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Are video_frames sequential?","metadata":{}},{"cell_type":"code","source":"print(\"Video frames sequential?\")\nfor seq_name in TRAIN_DF[\"sequence\"].unique():\n    sequential = True\n    numbers = TRAIN_DF[TRAIN_DF[\"sequence\"] == seq_name][\"video_frame\"].values\n    numbers.sort()\n    \n    i = numbers[0]\n    for num in numbers:\n        while i < num:\n            print(f\"Seq {seq_name}: {Fore.RED}Missing {i}{Fore.RESET}\")\n            i += 1\n        i += 1\n\n    if sequential:\n        print(f\"Seq {seq_name}: {Fore.GREEN}Yes{Fore.RESET}\")\nprint()\nnew_vid_ids = TRAIN_DF[\"video_id\"].astype(str) + \"-\" + TRAIN_DF[\"video_frame\"].astype(str)\nprint(\"How many images have strange image_ids:\", (TRAIN_DF[\"image_id\"] != new_vid_ids).sum())\n\n# Checking for the sequentiality of the video_frames","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.611645Z","iopub.execute_input":"2022-06-02T16:24:39.612455Z","iopub.status.idle":"2022-06-02T16:24:39.737583Z","shell.execute_reply.started":"2022-06-02T16:24:39.612410Z","shell.execute_reply":"2022-06-02T16:24:39.736824Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Do videos have unique sequence names?","metadata":{}},{"cell_type":"code","source":"vid_seq_pairs = TRAIN_DF[[\"video_id\", \"sequence\"]].drop_duplicates()\nrepeated_sequence_count = (vid_seq_pairs[\"sequence\"].value_counts() != 1).sum()\nprint(\"How many repeated sequences:\", repeated_sequence_count)\n\n# Checking for the duplicate sequence_ids","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.740870Z","iopub.execute_input":"2022-06-02T16:24:39.741114Z","iopub.status.idle":"2022-06-02T16:24:39.754769Z","shell.execute_reply.started":"2022-06-02T16:24:39.741085Z","shell.execute_reply":"2022-06-02T16:24:39.753801Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## How are the annotations distributed?","metadata":{}},{"cell_type":"code","source":"if \"detection_count\" not in TRAIN_DF.columns:\n    det_counts = TRAIN_DF.apply(lambda row: len(eval(row.annotations)), axis=1)\n    TRAIN_DF[\"detection_count\"] = det_counts\n    \n# Detection_count column contains the number of starfish present in that particular image","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:39.756039Z","iopub.execute_input":"2022-06-02T16:24:39.756536Z","iopub.status.idle":"2022-06-02T16:24:40.437437Z","shell.execute_reply.started":"2022-06-02T16:24:39.756495Z","shell.execute_reply":"2022-06-02T16:24:40.436647Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF[\"detection_count\"].value_counts()\n\n# Gives the frequency of each values","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:40.438551Z","iopub.execute_input":"2022-06-02T16:24:40.439100Z","iopub.status.idle":"2022-06-02T16:24:40.446622Z","shell.execute_reply.started":"2022-06-02T16:24:40.439055Z","shell.execute_reply":"2022-06-02T16:24:40.445925Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# 0 starfish is present in 18582 images, 1 starfish is present in 2801 images and so on","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:40.448038Z","iopub.execute_input":"2022-06-02T16:24:40.448465Z","iopub.status.idle":"2022-06-02T16:24:40.461065Z","shell.execute_reply.started":"2022-06-02T16:24:40.448425Z","shell.execute_reply":"2022-06-02T16:24:40.460170Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"bin_count = len(TRAIN_DF[\"detection_count\"].unique())\nplot = TRAIN_DF.hist(column=\"detection_count\", figsize=(16,6), bins=bin_count)\nax = plot[0][0]\nax.set_title(\"Starfish count, per image\")\n\n# X axis shows the number of starfish count and y axis shows the number of images containing those amount of starfish","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:40.463808Z","iopub.execute_input":"2022-06-02T16:24:40.464423Z","iopub.status.idle":"2022-06-02T16:24:40.813636Z","shell.execute_reply.started":"2022-06-02T16:24:40.464376Z","shell.execute_reply":"2022-06-02T16:24:40.812839Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"TRAIN_DF_WITH_STARFISH = TRAIN_DF[TRAIN_DF[\"detection_count\"] > 0]\nbin_count = len(TRAIN_DF_WITH_STARFISH[\"detection_count\"].unique())\nplot = TRAIN_DF_WITH_STARFISH.hist(column=\"detection_count\", figsize=(16,4), bins=bin_count)\nax = plot[0][0]\nax.set_title(\"Starfish count per image \");\n\n# Plotted only for images having atleast 1 starfish","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:40.814912Z","iopub.execute_input":"2022-06-02T16:24:40.815862Z","iopub.status.idle":"2022-06-02T16:24:41.097721Z","shell.execute_reply.started":"2022-06-02T16:24:40.815822Z","shell.execute_reply":"2022-06-02T16:24:41.096959Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## How do detections change during the frame?","metadata":{}},{"cell_type":"code","source":"import math \n\nSEQUENCE_COUNT = len(TRAIN_DF[\"sequence\"].drop_duplicates())\nFIG_COLS = 2\nFIG_ROWS = math.ceil(SEQUENCE_COUNT / FIG_COLS)\nfig = plt.figure(figsize=(30, 40), constrained_layout=True)\n\ndet_data = TRAIN_DF[[\"sequence\", \"video_id\", \"sequence_frame\", \"detection_count\"]].drop_duplicates()\nfor i, seq_num in enumerate(det_data[\"sequence\"].unique()): \n    seq_data = det_data[det_data[\"sequence\"] == seq_num].sort_values(by=\"sequence_frame\")\n    seq_data = seq_data.set_index(seq_data[\"sequence_frame\"]).drop(columns=\"sequence_frame\")\n    video_id = seq_data[\"video_id\"].iloc[0]\n    \n    col = (i % FIG_COLS) + 1\n    row = (i // FIG_COLS) + 1\n    \n    ax = plt.subplot(FIG_ROWS, FIG_COLS, i+1)\n    ax = seq_data[\"detection_count\"].plot.line(ax=ax)\n    ax.set_title(f\"Video {video_id}, Sequence {seq_num}\", fontsize=22)\n    ax.set_ylabel('Detections', fontsize=16)\n    ax.set_xlabel('Sequence Frame', fontsize=16)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:41.099076Z","iopub.execute_input":"2022-06-02T16:24:41.100076Z","iopub.status.idle":"2022-06-02T16:24:47.401210Z","shell.execute_reply.started":"2022-06-02T16:24:41.100031Z","shell.execute_reply":"2022-06-02T16:24:47.400493Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Above diagram shows that\n# -> For each subplot, there is a sequence_frame which starts from 0\n# -> For each sequence_frame, there will be image associated with that\n# -> For each image, there are list of annotations\n# -> Length of each annotations are plotted in the graph","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:47.402174Z","iopub.execute_input":"2022-06-02T16:24:47.402457Z","iopub.status.idle":"2022-06-02T16:24:47.407378Z","shell.execute_reply.started":"2022-06-02T16:24:47.402420Z","shell.execute_reply":"2022-06-02T16:24:47.406468Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"img_idx = TRAIN_DF.iloc[dict_seq_id_to_idxs[26651][0]].values\nimg_idx","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:47.409261Z","iopub.execute_input":"2022-06-02T16:24:47.409592Z","iopub.status.idle":"2022-06-02T16:24:47.424887Z","shell.execute_reply.started":"2022-06-02T16:24:47.409537Z","shell.execute_reply":"2022-06-02T16:24:47.423973Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.patches as patches","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:47.428745Z","iopub.execute_input":"2022-06-02T16:24:47.429000Z","iopub.status.idle":"2022-06-02T16:24:47.433891Z","shell.execute_reply.started":"2022-06-02T16:24:47.428965Z","shell.execute_reply":"2022-06-02T16:24:47.433055Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"rect_coordinates = ast.literal_eval(img_idx[5])\nrect_coordinates\n# Convert from string in list to normal list","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:47.435156Z","iopub.execute_input":"2022-06-02T16:24:47.436054Z","iopub.status.idle":"2022-06-02T16:24:47.447416Z","shell.execute_reply.started":"2022-06-02T16:24:47.436014Z","shell.execute_reply":"2022-06-02T16:24:47.446246Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"image = plt.imread(\"../input/tensorflow-great-barrier-reef/train_images/video_2/4718.jpg\")\n# plt.imshow(image)\nfig, ax = plt.subplots(1,figsize=(15,15))\nax.imshow(image)\nfor e in rect_coordinates:\n    x = e['x']\n    y = e['y']\n    w = e['width']\n    h = e['height']\n    rect = patches.Rectangle((x, y), w, h, linewidth=1,edgecolor='r', facecolor=\"none\")\n    ax.add_patch(rect)\nplt.show()\n\n# Plotting image having one starfish","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:47.450025Z","iopub.execute_input":"2022-06-02T16:24:47.450928Z","iopub.status.idle":"2022-06-02T16:24:48.115471Z","shell.execute_reply.started":"2022-06-02T16:24:47.450889Z","shell.execute_reply":"2022-06-02T16:24:48.114835Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"dict_seqid_to_cnt","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.116872Z","iopub.execute_input":"2022-06-02T16:24:48.117313Z","iopub.status.idle":"2022-06-02T16:24:48.123210Z","shell.execute_reply.started":"2022-06-02T16:24:48.117274Z","shell.execute_reply":"2022-06-02T16:24:48.122505Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"img_idx_1 = TRAIN_DF.iloc[19616].values\nimg_idx_1","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.124598Z","iopub.execute_input":"2022-06-02T16:24:48.125111Z","iopub.status.idle":"2022-06-02T16:24:48.137030Z","shell.execute_reply.started":"2022-06-02T16:24:48.125070Z","shell.execute_reply":"2022-06-02T16:24:48.136183Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"rect_coordinates_ = ast.literal_eval(img_idx_1[5])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.138377Z","iopub.execute_input":"2022-06-02T16:24:48.140239Z","iopub.status.idle":"2022-06-02T16:24:48.150525Z","shell.execute_reply.started":"2022-06-02T16:24:48.140119Z","shell.execute_reply":"2022-06-02T16:24:48.149571Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"rect_coordinates_","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.152105Z","iopub.execute_input":"2022-06-02T16:24:48.153121Z","iopub.status.idle":"2022-06-02T16:24:48.170084Z","shell.execute_reply.started":"2022-06-02T16:24:48.153082Z","shell.execute_reply":"2022-06-02T16:24:48.169214Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"image = plt.imread(\"../input/tensorflow-great-barrier-reef/train_images/video_2/5714.jpg\")\nfig, ax = plt.subplots(1,figsize=(15,15))\nax.imshow(image)\nfor e in rect_coordinates_:\n    x = e['x']\n    y = e['y']\n    w = e['width']\n    h = e['height']\n    rect = patches.Rectangle((x, y), w, h, linewidth=2,edgecolor='r', facecolor=\"none\")\n    ax.add_patch(rect)\nplt.show()\n\n# Plotting image having more than one starfish","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.171690Z","iopub.execute_input":"2022-06-02T16:24:48.172328Z","iopub.status.idle":"2022-06-02T16:24:48.819197Z","shell.execute_reply.started":"2022-06-02T16:24:48.172285Z","shell.execute_reply":"2022-06-02T16:24:48.815683Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"image.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.820632Z","iopub.execute_input":"2022-06-02T16:24:48.821082Z","iopub.status.idle":"2022-06-02T16:24:48.826815Z","shell.execute_reply.started":"2022-06-02T16:24:48.821044Z","shell.execute_reply":"2022-06-02T16:24:48.826176Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"plt.imread(\"../input/tensorflow-great-barrier-reef/train_images/video_2/1000.jpg\").shape","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.828262Z","iopub.execute_input":"2022-06-02T16:24:48.828830Z","iopub.status.idle":"2022-06-02T16:24:48.903068Z","shell.execute_reply.started":"2022-06-02T16:24:48.828788Z","shell.execute_reply":"2022-06-02T16:24:48.902251Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# clone the tensorflow/models repository for the pretrained models execution\n\n!git clone https://github.com/tensorflow/models\n    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n!cd models && git checkout ac8d06519","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:24:48.904466Z","iopub.execute_input":"2022-06-02T16:24:48.904912Z","iopub.status.idle":"2022-06-02T16:25:13.235746Z","shell.execute_reply.started":"2022-06-02T16:24:48.904873Z","shell.execute_reply":"2022-06-02T16:25:13.234790Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"%%bash\ncd models/research\n\n# Compile protos.\nprotoc object_detection/protos/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow to 2.6.0 to make it compatible with the CUDA version preinstalled on Kaggle.\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection/packages/tf2/setup.py .\nwget https://storage.googleapis.com/odml-dataset/others/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection/builders/model_builder_tf2_test.py","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:25:13.237720Z","iopub.execute_input":"2022-06-02T16:25:13.238040Z","iopub.status.idle":"2022-06-02T16:26:59.142010Z","shell.execute_reply.started":"2022-06-02T16:25:13.237998Z","shell.execute_reply":"2022-06-02T16:26:59.140971Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\n\nfrom PIL import Image, ImageDraw\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '/kaggle/input/tensorflow-great-barrier-reef/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:26:59.143708Z","iopub.execute_input":"2022-06-02T16:26:59.144349Z","iopub.status.idle":"2022-06-02T16:27:00.904909Z","shell.execute_reply.started":"2022-06-02T16:26:59.144302Z","shell.execute_reply":"2022-06-02T16:27:00.903989Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:00.906691Z","iopub.execute_input":"2022-06-02T16:27:00.907323Z","iopub.status.idle":"2022-06-02T16:27:01.626982Z","shell.execute_reply.started":"2022-06-02T16:27:00.907280Z","shell.execute_reply":"2022-06-02T16:27:01.626221Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"TRAINING_RATIO = 0.8\n\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(data_df))\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\n# Shuffle both the training and validation datasets.\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', \n      float(len(train_data_df)) / (len(train_data_df) + len(val_data_df)))\nprint('Training ratio (positive samples):', \n      float(train_positive_count) / (train_positive_count + val_positive_count))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:01.633011Z","iopub.execute_input":"2022-06-02T16:27:01.633401Z","iopub.status.idle":"2022-06-02T16:27:01.798310Z","shell.execute_reply.started":"2022-06-02T16:27:01.633357Z","shell.execute_reply":"2022-06-02T16:27:01.797503Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"train_data_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:01.799698Z","iopub.execute_input":"2022-06-02T16:27:01.800163Z","iopub.status.idle":"2022-06-02T16:27:01.813020Z","shell.execute_reply.started":"2022-06-02T16:27:01.800125Z","shell.execute_reply":"2022-06-02T16:27:01.812067Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Total length of the train_data\nprint(\"Length of training data: \"+ str(len(train_data_df)))\n#Total length of the validation data\nprint(\"Length of validation data: \"+ str(len(val_data_df)))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:01.814754Z","iopub.execute_input":"2022-06-02T16:27:01.815091Z","iopub.status.idle":"2022-06-02T16:27:01.820951Z","shell.execute_reply.started":"2022-06-02T16:27:01.815051Z","shell.execute_reply":"2022-06-02T16:27:01.819965Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Data points with atleast one starfish in training dataset\ntrain_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\n# Data points with atleast one starfish in validation dataset\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:01.822864Z","iopub.execute_input":"2022-06-02T16:27:01.823198Z","iopub.status.idle":"2022-06-02T16:27:01.842051Z","shell.execute_reply.started":"2022-06-02T16:27:01.823158Z","shell.execute_reply":"2022-06-02T16:27:01.841210Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Function to convert the images into TFRecord which is the binary format understandble by the pretrained model\n\nfrom object_detection.utils import dataset_util\nfrom object_detection.dataset_tools import tf_record_creation_util\n\n\ndef create_tf_example(video_id, video_frame, data_df, image_path):\n    \"\"\"Create a tf.Example entry for a given training image.\"\"\"\n    full_path = os.path.join(image_path, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] / width) \n            xmaxs.append((annotation['x'] + annotation['width']) / width) \n            ymins.append(annotation['y'] / height) \n            ymaxs.append((annotation['y'] + annotation['height']) / height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    \n    return tf_example\n\ndef convert_to_tfrecord(data_df, tfrecord_filebase, image_path, num_shards = 10):\n    \"\"\"Convert the object detection dataset to TFRecord as required by the TF ODT API.\"\"\"\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, tfrecord_filebase, num_shards)\n        \n        for index, row in data_df.iterrows():\n            if index % 500 == 0:\n                print('Processed {0} images.'.format(index))\n            tf_example = create_tf_example(row.video_id, row.video_frame, data_df, image_path)\n            output_shard_index = index % num_shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n        \n        print('Completed processing {0} images.'.format(len(data_df)))\n\n!mkdir dataset\nimage_path = os.path.join(INPUT_DIR, 'train_images')\n\n# Convert train images to TFRecord\nprint('Converting TRAIN images...')\nconvert_to_tfrecord(\n  train_data_df,\n  'dataset/cots_train',\n  image_path,\n  num_shards = 4\n)\n\n# Convert validation images to TFRecord\nprint('Converting VALIDATION images...')\nconvert_to_tfrecord(\n  val_data_df,\n  'dataset/cots_val',\n  image_path,\n  num_shards = 4\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:27:01.843490Z","iopub.execute_input":"2022-06-02T16:27:01.844285Z","iopub.status.idle":"2022-06-02T16:28:50.588139Z","shell.execute_reply.started":"2022-06-02T16:27:01.844243Z","shell.execute_reply":"2022-06-02T16:28:50.586126Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Labels mapping for mapping the output categories to the definite classess\n# Here only detections are done hence only one label is required\n\nlabel_map_str = \"\"\"item {\n  id: 1\n  name: 'COTS'\n}\"\"\"\n\nwith open('dataset/label_map.pbtxt', 'w') as f:\n  f.write(label_map_str)\n\n!more dataset/label_map.pbtxt","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:24.281063Z","iopub.execute_input":"2022-06-02T16:30:24.281907Z","iopub.status.idle":"2022-06-02T16:30:25.016736Z","shell.execute_reply.started":"2022-06-02T16:30:24.281859Z","shell.execute_reply":"2022-06-02T16:30:25.015723Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Getting the pretrained efficient-det-D0 model and decompressing it\n\n!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n!tar -xvzf efficientdet_d0_coco17_tpu-32.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:30.832453Z","iopub.execute_input":"2022-06-02T16:30:30.833503Z","iopub.status.idle":"2022-06-02T16:30:33.237264Z","shell.execute_reply.started":"2022-06-02T16:30:30.833448Z","shell.execute_reply":"2022-06-02T16:30:33.236259Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# This file is solely responsible for training,validating and predicting and detecting\n# Number of changes needed to be done on this config file for the transfer learning to incorporate\n\nfrom string import Template\n\nconfig_file_template = \"\"\"\n# SSD with EfficientNet-b0 + BiFPN feature extractor,\n# shared box predictor and focal loss (a.k.a EfficientDet-d0).\n# See EfficientDet, Tan et al, https://arxiv.org/abs/1911.09070\n# See Lin et al, https://arxiv.org/abs/1708.02002\n# Initialized from an EfficientDet-D0 checkpoint.\n#\n# Train on GPU\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    add_background_class: false\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 3\n      }\n    }\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 1280\n        max_dimension: 1280\n        pad_to_max_dimension: true\n        }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 64\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          force_use_bias: true\n          activation: SWISH\n          regularizer {\n            l2_regularizer {\n              weight: 0.00004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true\n            decay: 0.99\n            epsilon: 0.001\n          }\n        }\n        num_layers_before_predictor: 3\n        kernel_size: 3\n        use_depthwise: true\n      }\n    }\n    feature_extractor {\n      type: 'ssd_efficientnet-b0_bifpn_keras'\n      bifpn {\n        min_level: 3\n        max_level: 7\n        num_iterations: 3\n        num_filters: 64\n      }\n      conv_hyperparams {\n        force_use_bias: true\n        activation: SWISH\n        regularizer {\n          l2_regularizer {\n            weight: 0.00004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.99,\n          epsilon: 0.001,\n        }\n      }\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 1.5\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.5\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint: \"efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0\"\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 2\n  sync_replicas: false\n  startup_delay_steps: 0\n  replicas_to_aggregate: 1\n  use_bfloat16: false\n  num_steps: $training_steps\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_scale_crop_and_pad_to_square {\n      output_size: 1280\n      scale_min: 0.5\n      scale_max: 2.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: $training_steps\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\n\ntrain_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"dataset/cots_train-?????-of-00004\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 2;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"dataset/cots_val-?????-of-00004\"\n  }\n}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:33.240065Z","iopub.execute_input":"2022-06-02T16:30:33.240411Z","iopub.status.idle":"2022-06-02T16:30:33.250278Z","shell.execute_reply.started":"2022-06-02T16:30:33.240367Z","shell.execute_reply":"2022-06-02T16:30:33.249304Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Changing the placeholders in config file\n\nTRAINING_STEPS = 10000\nWARMUP_STEPS = 2000\nPIPELINE_CONFIG_PATH='dataset/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:33.251765Z","iopub.execute_input":"2022-06-02T16:30:33.252677Z","iopub.status.idle":"2022-06-02T16:30:33.266158Z","shell.execute_reply.started":"2022-06-02T16:30:33.252637Z","shell.execute_reply":"2022-06-02T16:30:33.265141Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Training the model with pipeline config path and model directory to store the checkpoints created during the training which are helpful for evaluating\n\nMODEL_DIR='cots_efficientdet_d0'\n!mkdir {MODEL_DIR}\n!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:33.268915Z","iopub.execute_input":"2022-06-02T16:30:33.269962Z","iopub.status.idle":"2022-06-02T17:12:34.356524Z","shell.execute_reply.started":"2022-06-02T16:30:33.269918Z","shell.execute_reply":"2022-06-02T17:12:34.355482Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model using the checkpoints created during the training step\n\n!python models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --checkpoint_dir={MODEL_DIR} \\\n    --eval_timeout=0 \\\n    --alsologtostderr","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:12:34.360826Z","iopub.execute_input":"2022-06-02T17:12:34.361084Z","iopub.status.idle":"2022-06-02T17:17:57.759845Z","shell.execute_reply.started":"2022-06-02T17:12:34.361053Z","shell.execute_reply":"2022-06-02T17:17:57.758922Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# Saving the model for the detections and the predictions with the test dataset\n\n!python models/research/object_detection/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --trained_checkpoint_dir={MODEL_DIR} \\\n    --output_directory={MODEL_DIR}/output","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:17:57.761624Z","iopub.execute_input":"2022-06-02T17:17:57.762077Z","iopub.status.idle":"2022-06-02T17:20:08.535305Z","shell.execute_reply.started":"2022-06-02T17:17:57.762038Z","shell.execute_reply":"2022-06-02T17:20:08.534369Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"!ls {MODEL_DIR}/output","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:08.537157Z","iopub.execute_input":"2022-06-02T17:20:08.537476Z","iopub.status.idle":"2022-06-02T17:20:09.279360Z","shell.execute_reply.started":"2022-06-02T17:20:08.537425Z","shell.execute_reply":"2022-06-02T17:20:09.278069Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# Load the TensorFlow COTS detection model into memory.\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:09.281609Z","iopub.execute_input":"2022-06-02T17:20:09.282000Z","iopub.status.idle":"2022-06-02T17:20:39.456237Z","shell.execute_reply.started":"2022-06-02T17:20:09.281957Z","shell.execute_reply":"2022-06-02T17:20:39.455443Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:39.457710Z","iopub.execute_input":"2022-06-02T17:20:39.458009Z","iopub.status.idle":"2022-06-02T17:20:39.468049Z","shell.execute_reply.started":"2022-06-02T17:20:39.457973Z","shell.execute_reply":"2022-06-02T17:20:39.467069Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Load the test image\nimgTest = load_image_into_numpy_array(\"../input/tensorflow-great-barrier-reef/train_images/video_2/5714.jpg\")\nimgTest.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:39.472059Z","iopub.execute_input":"2022-06-02T17:20:39.472288Z","iopub.status.idle":"2022-06-02T17:20:41.609378Z","shell.execute_reply.started":"2022-06-02T17:20:39.472261Z","shell.execute_reply":"2022-06-02T17:20:41.608611Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"listTest = [imgTest]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:41.610646Z","iopub.execute_input":"2022-06-02T17:20:41.611073Z","iopub.status.idle":"2022-06-02T17:20:41.617293Z","shell.execute_reply.started":"2022-06-02T17:20:41.611033Z","shell.execute_reply":"2022-06-02T17:20:41.616439Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"DETECTION_THRESHOLD = 0.3\n\n# Check for the predictions\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor image_np in listTest:\n    height, width, _ = image_np.shape\n    \n    # Run object detection using the TensorFlow model.\n    detections = detect(image_np)\n    \n    # Parse the detection result and generate a prediction string.\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    # Generate the submission data.\n    prediction_str = ' '.join(predictions)\n#     sample_prediction_df['annotations'] = prediction_str\n\n    print('Prediction:', prediction_str)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:22:26.887485Z","iopub.execute_input":"2022-06-02T17:22:26.887785Z","iopub.status.idle":"2022-06-02T17:22:27.039249Z","shell.execute_reply.started":"2022-06-02T17:22:26.887734Z","shell.execute_reply":"2022-06-02T17:22:27.038449Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Clear the saved directories \n\n!rm -rf dataset\n!rm -rf train_images\n!rm tensorflow-great-barrier-reef.zip\n\n# Remove other data downloaded during training.\n!rm -rf models\n!rm efficientdet_d0_coco17_tpu-32.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-06-02T17:20:46.082720Z","iopub.status.idle":"2022-06-02T17:20:46.083606Z","shell.execute_reply.started":"2022-06-02T17:20:46.083352Z","shell.execute_reply":"2022-06-02T17:20:46.083384Z"},"trusted":true},"execution_count":null,"outputs":[]}]}